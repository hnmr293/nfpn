import torch

# 0000_1111
HF8_MAX = 0.5 * (1+0.5)

# 0000_0000
HF8_MIN = 2 ** (-15)


def to_hf8(data: torch.Tensor):
    data = data.to(dtype=torch.float16)
    
    assert (data.abs() <= HF8_MAX).all().item(), f'max value = {data.abs().max().item()}'
    
    # fp16: sEEE_EEff_ffff_ffff
    #
    # hf8 type-a: sEEE_ffff  where EEE != 000
    #       mantissa = E-12 = -5..-11
    # hf8 type-b: s000_f0GG
    #       mantissa = G-15 = -12..-15
    # hf8 type-c: s000_f1HH
    #       mantissa = H-4 = -1..-4
    #
    # * significant bits
    #       0000 = 1 + 0
    #       1000 = 1 + 0.5
    #       0100 = 1 + 0.25
    #       0010 = 1 + 0.125
    #       0001 = 1 + 0.0625
    #
    
    idata = data.view(dtype=torch.int16).view(size=(-1,))
    
    mask_s = 0b1000_0000_0000_0000
    mask_e = 0b0111_1100_0000_0000
    mask_f = 0b0000_0011_1111_1111
    
    # sign
    s = (idata >> 8).to(dtype=torch.uint8) & 0b1000_0000
    
    # exponential
    e = ((idata & mask_e) >> 10) - 15
    e = e.clamp(-15, -1)
    
    E_mask = torch.logical_and(-11 <= e, e <= -5)
    
    HF8 = (e + 12).to(dtype=torch.int8).view(dtype=torch.uint8)
    #HF8[~E_mask] = 0
    HF8.bitwise_and_(0b111)
    HF8.bitwise_left_shift_(4)
    HF8.add_(s)
    
    # significants
    HF8.add_(((idata & mask_f) >> 6).to(dtype=torch.uint8))
    HF8[~E_mask] &= 0b1000_1000
    
    ## significants of type-b
    G_mask = e < -11
    Ge = (e + 15).to(dtype=torch.int8).view(dtype=torch.uint8)
    Ge = Ge.clamp(0, 3)
    HF8[G_mask] += Ge[G_mask]
    
    ## significants of type-c
    H_mask = -5 < e
    He = (e + 4).to(dtype=torch.int8).view(dtype=torch.uint8)
    He = He.clamp(0, 3)
    HF8[H_mask] += He[H_mask] + 0b100
    
    return HF8

def hf8_to_fp16_2(hf8: torch.Tensor):
    assert hf8.dtype == torch.uint8
    assert hf8.ndim == 1
    
    FP16 = torch.zeros(hf8.shape, dtype=torch.int16, device=hf8.device)
    
    # sign
    FP16[:] = hf8 & 0b1000_0000
    FP16 <<= 8
    
    # exponential
    exp_e = ((hf8 & 0b0111_0000) >> 4).to(dtype=torch.int16)
    exp_gh = (hf8 & 0b11).to(dtype=torch.int16)
    
    E_mask = exp_e != 0
    G_mask = torch.logical_and(exp_e == 0, (hf8 & 0b100) == 0)
    H_mask = torch.logical_and(exp_e == 0, (hf8 & 0b100) == 4)
    
    FP16 += (
        ## type-a
        E_mask * (exp_e + 3) +
        #                 ^ EEE - 12 = -15 + a <=> a = EEE + 3
        ## type-b
        G_mask * exp_gh +
        #              ^ GG - 15 = -15 + a <=> a = GG
        ## type-c
        H_mask * (exp_gh + 11)
        #                  ^ HH - 4 = -15 + a <=> a = HH + 11
    ) << 10
    
    # significants
    FP16 += (
        ## type-a
        E_mask * (hf8 & 0b1111) +
        ## type-b and type-c
        (G_mask + H_mask) * (hf8 & 0b1000)
    ).to(dtype=torch.int16) << 6  # TODO pad random value?
    
    return FP16.view(dtype=torch.float16)


HF8_TO_FP16 = torch.tensor([
    0b0000000000000000, 0b0000010000000000, 0b0000100000000000, 0b0000110000000000, 0b0010110000000000, 0b0011000000000000, 0b0011010000000000, 0b0011100000000000, 0b0000001000000000, 0b0000011000000000, 0b0000101000000000, 0b0000111000000000, 0b0010111000000000, 0b0011001000000000, 0b0011011000000000, 0b0011101000000000, 
    0b0001000000000000, 0b0001000001000000, 0b0001000010000000, 0b0001000011000000, 0b0001000100000000, 0b0001000101000000, 0b0001000110000000, 0b0001000111000000, 0b0001001000000000, 0b0001001001000000, 0b0001001010000000, 0b0001001011000000, 0b0001001100000000, 0b0001001101000000, 0b0001001110000000, 0b0001001111000000, 
    0b0001010000000000, 0b0001010001000000, 0b0001010010000000, 0b0001010011000000, 0b0001010100000000, 0b0001010101000000, 0b0001010110000000, 0b0001010111000000, 0b0001011000000000, 0b0001011001000000, 0b0001011010000000, 0b0001011011000000, 0b0001011100000000, 0b0001011101000000, 0b0001011110000000, 0b0001011111000000, 
    0b0001100000000000, 0b0001100001000000, 0b0001100010000000, 0b0001100011000000, 0b0001100100000000, 0b0001100101000000, 0b0001100110000000, 0b0001100111000000, 0b0001101000000000, 0b0001101001000000, 0b0001101010000000, 0b0001101011000000, 0b0001101100000000, 0b0001101101000000, 0b0001101110000000, 0b0001101111000000, 
    0b0001110000000000, 0b0001110001000000, 0b0001110010000000, 0b0001110011000000, 0b0001110100000000, 0b0001110101000000, 0b0001110110000000, 0b0001110111000000, 0b0001111000000000, 0b0001111001000000, 0b0001111010000000, 0b0001111011000000, 0b0001111100000000, 0b0001111101000000, 0b0001111110000000, 0b0001111111000000, 
    0b0010000000000000, 0b0010000001000000, 0b0010000010000000, 0b0010000011000000, 0b0010000100000000, 0b0010000101000000, 0b0010000110000000, 0b0010000111000000, 0b0010001000000000, 0b0010001001000000, 0b0010001010000000, 0b0010001011000000, 0b0010001100000000, 0b0010001101000000, 0b0010001110000000, 0b0010001111000000, 
    0b0010010000000000, 0b0010010001000000, 0b0010010010000000, 0b0010010011000000, 0b0010010100000000, 0b0010010101000000, 0b0010010110000000, 0b0010010111000000, 0b0010011000000000, 0b0010011001000000, 0b0010011010000000, 0b0010011011000000, 0b0010011100000000, 0b0010011101000000, 0b0010011110000000, 0b0010011111000000, 
    0b0010100000000000, 0b0010100001000000, 0b0010100010000000, 0b0010100011000000, 0b0010100100000000, 0b0010100101000000, 0b0010100110000000, 0b0010100111000000, 0b0010101000000000, 0b0010101001000000, 0b0010101010000000, 0b0010101011000000, 0b0010101100000000, 0b0010101101000000, 0b0010101110000000, 0b0010101111000000, 
    0b0000000000000000, 0b0000010000000000, 0b0000100000000000, 0b0000110000000000, 0b0010110000000000, 0b0011000000000000, 0b0011010000000000, 0b0011100000000000, 0b0000001000000000, 0b0000011000000000, 0b0000101000000000, 0b0000111000000000, 0b0010111000000000, 0b0011001000000000, 0b0011011000000000, 0b0011101000000000, 
    0b0001000000000000, 0b0001000001000000, 0b0001000010000000, 0b0001000011000000, 0b0001000100000000, 0b0001000101000000, 0b0001000110000000, 0b0001000111000000, 0b0001001000000000, 0b0001001001000000, 0b0001001010000000, 0b0001001011000000, 0b0001001100000000, 0b0001001101000000, 0b0001001110000000, 0b0001001111000000, 
    0b0001010000000000, 0b0001010001000000, 0b0001010010000000, 0b0001010011000000, 0b0001010100000000, 0b0001010101000000, 0b0001010110000000, 0b0001010111000000, 0b0001011000000000, 0b0001011001000000, 0b0001011010000000, 0b0001011011000000, 0b0001011100000000, 0b0001011101000000, 0b0001011110000000, 0b0001011111000000, 
    0b0001100000000000, 0b0001100001000000, 0b0001100010000000, 0b0001100011000000, 0b0001100100000000, 0b0001100101000000, 0b0001100110000000, 0b0001100111000000, 0b0001101000000000, 0b0001101001000000, 0b0001101010000000, 0b0001101011000000, 0b0001101100000000, 0b0001101101000000, 0b0001101110000000, 0b0001101111000000, 
    0b0001110000000000, 0b0001110001000000, 0b0001110010000000, 0b0001110011000000, 0b0001110100000000, 0b0001110101000000, 0b0001110110000000, 0b0001110111000000, 0b0001111000000000, 0b0001111001000000, 0b0001111010000000, 0b0001111011000000, 0b0001111100000000, 0b0001111101000000, 0b0001111110000000, 0b0001111111000000, 
    0b0010000000000000, 0b0010000001000000, 0b0010000010000000, 0b0010000011000000, 0b0010000100000000, 0b0010000101000000, 0b0010000110000000, 0b0010000111000000, 0b0010001000000000, 0b0010001001000000, 0b0010001010000000, 0b0010001011000000, 0b0010001100000000, 0b0010001101000000, 0b0010001110000000, 0b0010001111000000, 
    0b0010010000000000, 0b0010010001000000, 0b0010010010000000, 0b0010010011000000, 0b0010010100000000, 0b0010010101000000, 0b0010010110000000, 0b0010010111000000, 0b0010011000000000, 0b0010011001000000, 0b0010011010000000, 0b0010011011000000, 0b0010011100000000, 0b0010011101000000, 0b0010011110000000, 0b0010011111000000, 
    0b0010100000000000, 0b0010100001000000, 0b0010100010000000, 0b0010100011000000, 0b0010100100000000, 0b0010100101000000, 0b0010100110000000, 0b0010100111000000, 0b0010101000000000, 0b0010101001000000, 0b0010101010000000, 0b0010101011000000, 0b0010101100000000, 0b0010101101000000, 0b0010101110000000, 0b0010101111000000, 
], dtype=torch.int16)
# torch does not have torch.uint16 (x_x)
HF8_TO_FP16 = HF8_TO_FP16.view((2,-1))
HF8_TO_FP16[1,:].bitwise_or_(0x8000)
HF8_TO_FP16 = HF8_TO_FP16.view((-1,))

def hf8_to_fp16(hf8: torch.Tensor):
    global HF8_TO_FP16
    
    assert hf8.dtype == torch.uint8
    assert hf8.ndim == 1

    if HF8_TO_FP16.device != hf8.device:
        HF8_TO_FP16 = HF8_TO_FP16.to(hf8.device)
    
    FP16 = torch.take(HF8_TO_FP16, hf8.long())
    
    return FP16.view(dtype=torch.float16)
